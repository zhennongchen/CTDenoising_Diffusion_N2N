{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have Github copilot installed, search it in the VSCode extension marketplace, it will make your coding much easier\n",
    "import sys \n",
    "sys.path.append('/host/d/Github/')\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "import CTDenoising_Diffusion_N2N.noise2noise.model as noise2noise\n",
    "import CTDenoising_Diffusion_N2N.functions_collection as ff\n",
    "import CTDenoising_Diffusion_N2N.Build_lists.Build_list as Build_list\n",
    "import CTDenoising_Diffusion_N2N.noise2noise.Generator as Generator\n",
    "\n",
    "# if it says no module named ... (e.g., lpips), do the following:\n",
    "# 1. go to powershell, type wsl, enter wsl\n",
    "# 2. in wsl, type: sudo docker container ls, you will see the container id of your running container\n",
    "# 3. type: sudo docker container exec -it -u 0 <container_id> bash\n",
    "# 4. now you are inside the container root, type: pip install lpips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: define pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = 'noise2noise'\n",
    "epoch = 78\n",
    "# define your own saved model path and prediction save path\n",
    "trained_model_filename = os.path.join('/host/d/projects/denoising/models', trial_name, 'models/model-' + str(epoch)+ '.pt')\n",
    "save_folder = os.path.join('/host/d/projects/denoising/models', trial_name, 'pred_images'); os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters no need to change\n",
    "image_size = [512,512]\n",
    "\n",
    "histogram_equalization = True\n",
    "background_cutoff = -1000\n",
    "maximum_cutoff = 2000\n",
    "normalize_factor = 'equation' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: build patient list - testing, batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example excel path (you can find it in this repo - example data folder)\n",
    "# build_sheet =  Build_list.Build(os.path.join('/host/d/Github/CTDenoising_Diffusion_N2N/example_data/patient_lists/patient_list_supervised.xlsx'))\n",
    "# use the spreadsheet I gave to you and put the path here:\n",
    "build_sheet =  Build_list.Build(os.path.join('/host/d/projects/denoising/Patient_lists/fixedCT_static_simulation_train_test_gaussian_local.xlsx'))\n",
    "\n",
    "_,patient_id_list,patient_subid_list,random_num_list, condition_list, x0_list = build_sheet.__build__(batch_list = [5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in out is :  [(16, 32), (32, 64), (64, 128), (128, 256)]\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = noise2noise.Unet2D(\n",
    "    init_dim = 16,\n",
    "    channels = 2, \n",
    "    out_dim = 1,\n",
    "    dim_mults = (2,4,8,16),\n",
    "    full_attn = (None,None, False, True),\n",
    "    act = 'ReLU',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4: main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load histogram equalization pre-saved files\n",
    "# change to your own path, they are in this repo - example data folder\n",
    "bins = np.load('/host/d/Github/CTDenoising_Diffusion_N2N/example_data/histogram_equalization/bins.npy') \n",
    "bins_mapped = np.load('/host/d/Github/CTDenoising_Diffusion_N2N/example_data/histogram_equalization/bins_mapped.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 00004038 0000455420 0\n",
      "histogram equalization:  True\n",
      "model device:  cuda:0\n",
      "reference_img shape:  (512, 512, 50)\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "for i in range(0, 1):#patient_id_list.shape[0]):\n",
    "    patient_id = patient_id_list[i]\n",
    "    patient_subid = patient_subid_list[i]\n",
    "    random_num = random_num_list[i]\n",
    "    x0_file = x0_list[i]\n",
    "    condition_file = condition_list[i]\n",
    "\n",
    "    print(i,patient_id, patient_subid, random_num)\n",
    "\n",
    "    # # get the ground truth image\n",
    "    # gt_img = nb.load(x0_file)\n",
    "    # affine = gt_img.affine; gt_img = gt_img.get_fdata()[:,:,30:80]\n",
    "\n",
    "    # get the condition image\n",
    "    condition_img = nb.load(condition_file).get_fdata()[:,:,30:80]\n",
    "    affine = nb.load(condition_file).affine\n",
    "    \n",
    "\n",
    "    # make folders\n",
    "    ff.make_folder([os.path.join(save_folder, patient_id), os.path.join(save_folder, patient_id, patient_subid), os.path.join(save_folder, patient_id, patient_subid, 'random_' + str(random_num))])\n",
    "    save_folder_case = os.path.join(save_folder, patient_id, patient_subid, 'random_' + str(random_num), 'epoch' + str(epoch)); os.makedirs(save_folder_case, exist_ok=True)\n",
    "\n",
    "    # save condition image\n",
    "    nb.save(nb.Nifti1Image(condition_img, affine), os.path.join(save_folder_case,'condition_img.nii.gz'))\n",
    "\n",
    "    # # generator\n",
    "    generator = Generator.Dataset_2D(\n",
    "        img_list = np.array([condition_file]),\n",
    "        image_size = image_size,\n",
    "\n",
    "        num_slices_per_image = 50,\n",
    "        random_pick_slice = False,\n",
    "        slice_range = [30,80],\n",
    "\n",
    "        bins = bins,\n",
    "        bins_mapped = bins_mapped,\n",
    "        histogram_equalization = histogram_equalization,\n",
    "        background_cutoff = background_cutoff,\n",
    "        maximum_cutoff = maximum_cutoff,\n",
    "        normalize_factor = normalize_factor,)\n",
    "\n",
    "    # # sample:\n",
    "    sampler = noise2noise.Sampler(model,generator,batch_size = 1, image_size = image_size)\n",
    "\n",
    "    pred_img = sampler.sample_2D(trained_model_filename, condition_img)\n",
    "    pred_img_final = pred_img\n",
    "    \n",
    "    # save\n",
    "    nb.save(nb.Nifti1Image(pred_img_final, affine), os.path.join(save_folder_case, 'pred_img.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
