{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: analyze the bias\n",
    "\n",
    "- calculate the mean pixel value of the brain tissue (within a ROI) for each image.\n",
    "\n",
    "## Task 2: metric calculation\n",
    "\n",
    "- calculate MAE, RMSE, SSIM and LPIPS.\n",
    "\n",
    "## Task 3: analyze the effect of the number of inference averaged\n",
    "\n",
    "- calculate the MAE, RMSE, SSIM and LPIPS with 2,4,6,8,10,12,14,16,18,20 inferences averaged\n",
    "\n",
    "---\n",
    "\n",
    "### Docker environment\n",
    "Please use `docker/docker_pytorch`, it will build a tensorflow docker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/Documents')\n",
    "import os\n",
    "import torch\n",
    "import lpips\n",
    "import numpy as np \n",
    "import nibabel as nb\n",
    "import pandas as pd\n",
    "from scipy.ndimage import binary_erosion, generate_binary_structure\n",
    "import CTDenoising_Diffusion_N2N.functions_collection as ff\n",
    "import CTDenoising_Diffusion_N2N.Build_lists.Build_list as Build_list\n",
    "import CTDenoising_Diffusion_N2N.Data_processing as Data_processing\n",
    "\n",
    "main_path = '/mnt/camca_NAS/denoising/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a patient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_sheet =  Build_list.Build(os.path.join('/mnt/camca_NAS/denoising/Patient_lists/fixedCT_static_simulation_train_test_gaussian_NAS.xlsx'))\n",
    "_,patient_id_list,patient_subid_list,random_num_list, condition_list, x0_list = build_sheet.__build__(batch_list = [5]) \n",
    "n = ff.get_X_numbers_in_interval(total_number = patient_id_list.shape[0],start_number = 0,end_number = 1, interval = 2) # each case has two simulations, we do on the first one as example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 and 2\n",
    "output:\n",
    "- ```mean_measurements.xlsx``` for task 1\n",
    "- ```quantitative_results.xlsx``` for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []; results_mean = []\n",
    "for i in range(0,n.shape[0]):\n",
    "    patient_id = patient_id_list[n[i]]\n",
    "    patient_subid = patient_subid_list[n[i]]\n",
    "    random_n = random_num_list[n[i]]\n",
    "    print(patient_id, patient_subid, random_n)\n",
    "\n",
    "    # reference image\n",
    "    gt_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta0/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/gt_img.nii.gz')\n",
    "    gt_img = nb.load(gt_file).get_fdata()\n",
    "    gt_img_brain = Data_processing.cutoff_intensity(gt_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # noisy image\n",
    "    condition_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta0/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/condition_img.nii.gz')\n",
    "    condition_img = nb.load(condition_file).get_fdata()\n",
    "    condition_img_brain = Data_processing.cutoff_intensity(condition_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # noise2noise\n",
    "    noise2noise_file = os.path.join('/mnt/camca_NAS/denoising/models/noise2noise/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch78/pred_img.nii.gz')\n",
    "    noise2noise_img = nb.load(noise2noise_file).get_fdata() \n",
    "    noise2noise_img_brain = Data_processing.cutoff_intensity(noise2noise_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # supervised method\n",
    "    supervised_file = os.path.join('/mnt/camca_NAS/denoising/models/supervised_possion/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch58_1/pred_img.nii.gz')\n",
    "    supervised_img = nb.load(supervised_file).get_fdata() \n",
    "    supervised_img_brain = Data_processing.cutoff_intensity(supervised_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # supervised_avg_file = os.path.join('/mnt/camca_NAS/denoising/models/supervised_DDPM_possion_2D/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch50final/pred_img.nii.gz')\n",
    "    # supervised_avg_img = nb.load(supervised_avg_file).get_fdata() \n",
    "    # supervised_avg_img_brain = Data_processing.cutoff_intensity(supervised_avg_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # our method (unsupervised), beta = 0, 1 inference\n",
    "    unsupervised_beta0_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta0/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/pred_img.nii.gz')\n",
    "    unsupervised_beta0_img = nb.load(unsupervised_beta0_file).get_fdata()\n",
    "    unsupervised_beta0_img_brain = Data_processing.cutoff_intensity(unsupervised_beta0_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # our method (unsupervised), beta = 0, 10 inference\n",
    "    unsupervised_beta0_avg10_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta0/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61avg/pred_img_scans10.nii.gz')\n",
    "    unsupervised_beta0_avg10_img = nb.load(unsupervised_beta0_avg10_file).get_fdata()\n",
    "    unsupervised_beta0_avg10_img_brain = Data_processing.cutoff_intensity(unsupervised_beta0_avg10_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # our method (unsupervised), beta = 0, 20 inference\n",
    "    unsupervised_beta0_avg20_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta0/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61avg/pred_img_scans20.nii.gz')\n",
    "    unsupervised_beta0_avg20_img = nb.load(unsupervised_beta0_avg20_file).get_fdata()\n",
    "    unsupervised_beta0_avg20_img_brain = Data_processing.cutoff_intensity(unsupervised_beta0_avg20_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # our method (unsupervised), beta = 10, 1 inference\n",
    "    unsupervised_beta10_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta10/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch65_1/pred_img.nii.gz')\n",
    "    unsupervised_beta10_img = nb.load(unsupervised_beta10_file).get_fdata()\n",
    "    unsupervised_beta10_img_brain = Data_processing.cutoff_intensity(unsupervised_beta10_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # our method (unsupervised), beta = 10, 10 inference\n",
    "    unsupervised_beta10_avg10_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta10/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch65avg/pred_img_scans10.nii.gz')\n",
    "    unsupervised_beta10_avg10_img = nb.load(unsupervised_beta10_avg10_file).get_fdata()\n",
    "    unsupervised_beta10_avg10_img_brain = Data_processing.cutoff_intensity(unsupervised_beta10_avg10_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # our method (unsupervised), beta = 10, 20 inference\n",
    "    unsupervised_beta10_avg20_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta10/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch65avg/pred_img_scans20.nii.gz')\n",
    "    unsupervised_beta10_avg20_img = nb.load(unsupervised_beta10_avg20_file).get_fdata()\n",
    "    unsupervised_beta10_avg20_img_brain = Data_processing.cutoff_intensity(unsupervised_beta10_avg20_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    # our method (unsupervised), beta = 20, 1 inference\n",
    "    unsupervised_beta20_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta20/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch58_1/pred_img.nii.gz')\n",
    "    unsupervised_beta20_img = nb.load(unsupervised_beta20_file).get_fdata()\n",
    "    unsupervised_beta20_img_brain = Data_processing.cutoff_intensity(unsupervised_beta20_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    #### TASK 1: compare the mean value in brain region (crop a ROI in the center)\n",
    "    x,y = 256,256\n",
    "    mean_gt = np.mean(np.clip(gt_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_condition = np.mean(np.clip(condition_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_noise2noise = np.mean(np.clip(noise2noise_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_supervised= np.mean(np.clip(supervised_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_unsupervised_beta0 = np.mean(np.clip(unsupervised_beta0_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_unsupervised_beta0_avg10 = np.mean(np.clip(unsupervised_beta0_avg10_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_unsupervised_beta0_avg20 = np.mean(np.clip(unsupervised_beta0_avg20_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_unsupervised_beta10 = np.mean(np.clip(unsupervised_beta10_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_unsupervised_beta10_avg10 = np.mean(np.clip(unsupervised_beta10_avg10_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_unsupervised_beta10_avg20 = np.mean(np.clip(unsupervised_beta10_avg20_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "    mean_unsupervised_beta20 = np.mean(np.clip(unsupervised_beta20_img_brain[x-50: x+50, y-50: y+50, 20:40],0,100))\n",
    "\n",
    "    results_mean.append([patient_id, patient_subid, random_n,\n",
    "    mean_gt, mean_condition, mean_noise2noise, mean_supervised,\n",
    "    mean_unsupervised_beta0, mean_unsupervised_beta0_avg10, mean_unsupervised_beta0_avg20,\n",
    "    mean_unsupervised_beta10, mean_unsupervised_beta10_avg10, mean_unsupervised_beta10_avg20,\n",
    "    mean_unsupervised_beta20])\n",
    "    df_mean = pd.DataFrame(results_mean, columns = ['patient_id', 'patient_subid', 'random_n',\n",
    "    'mean_gt', 'mean_condition', 'mean_noise2noise', 'mean_supervised',\n",
    "    'mean_unsupervised_beta0', 'mean_unsupervised_beta0_avg10', 'mean_unsupervised_beta0_avg20',\n",
    "    'mean_unsupervised_beta10', 'mean_unsupervised_beta10_avg10', 'mean_unsupervised_beta10_avg20',\n",
    "    'mean_unsupervised_beta20'])\n",
    "    file_name = 'mean_measurements.xlsx'\n",
    "    df_mean.to_excel(os.path.join('/mnt/camca_NAS/denoising/models', file_name), index = False)\n",
    "\n",
    "    \n",
    "    #### TASK2: compare brain region metrics \n",
    "    # define eroded mask\n",
    "    mask = np.zeros(gt_img_brain.shape, dtype=bool)\n",
    "    mask[(gt_img_brain>0) & (gt_img_brain < 100)] = 1\n",
    "    structure = np.ones((6,6))\n",
    "    mask_eroded = np.zeros_like(mask, dtype=bool)\n",
    "    for i in range(mask.shape[2]):\n",
    "        mask_eroded[:, :, i] = binary_erosion(mask[:, :, i], structure=structure, iterations=1)\n",
    "\n",
    "    mae_motion, _, rmse_motion, _, ssim_motion,_ = ff.compare(condition_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "    mae_n2n, _, rmse_n2n, _, ssim_n2n, _ = ff.compare(noise2noise_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "    mae_supervised, _, rmse_supervised, _, ssim_supervised,_= ff.compare(supervised_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "\n",
    "    mae_unsupervised_beta0, _, rmse_unsupervised_beta0, _, ssim_unsupervised_beta0,_ = ff.compare(unsupervised_beta0_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "    mae_unsupervised_beta0_avg10, _, rmse_unsupervised_beta0_avg10, _, ssim_unsupervised_beta0_avg10,_ = ff.compare(unsupervised_beta0_avg10_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "    mae_unsupervised_beta0_avg20, _, rmse_unsupervised_beta0_avg20, _, ssim_unsupervised_beta0_avg20,_ = ff.compare(unsupervised_beta0_avg20_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "    mae_unsupervised_beta10, _, rmse_unsupervised_beta10, _, ssim_unsupervised_beta10,_ = ff.compare(unsupervised_beta10_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "    mae_unsupervised_beta10_avg10, _, rmse_unsupervised_beta10_avg10, _, ssim_unsupervised_beta10_avg10,_ = ff.compare(unsupervised_beta10_avg10_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "    mae_unsupervised_beta10_avg20, _, rmse_unsupervised_beta10_avg20, _, ssim_unsupervised_beta10_avg20,_ = ff.compare(unsupervised_beta10_avg20_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "    mae_unsupervised_beta20, _, rmse_unsupervised_beta20, _, ssim_unsupervised_beta20,_ = ff.compare(unsupervised_beta20_img_brain[mask_eroded==1], gt_img_brain[mask_eroded==1], cutoff_low = 0, cutoff_high = 100)\n",
    "\n",
    "\n",
    "    print('motion metrics: ', mae_motion, rmse_motion, ssim_motion)\n",
    "    print('n2n metrics: ', mae_n2n, rmse_n2n, ssim_n2n)\n",
    "    print('supervised metrics: ', mae_supervised, rmse_supervised, ssim_supervised)\n",
    "    print('unsupervised beta0 metrics: ', mae_unsupervised_beta0, rmse_unsupervised_beta0, ssim_unsupervised_beta0)\n",
    "    print('unsupervised beta0 avg10 metrics: ', mae_unsupervised_beta0_avg10, rmse_unsupervised_beta0_avg10, ssim_unsupervised_beta0_avg10)\n",
    "    print('unsupervised beta0 avg20 metrics: ', mae_unsupervised_beta0_avg20, rmse_unsupervised_beta0_avg20, ssim_unsupervised_beta0_avg20)\n",
    "    print('unsupervised beta10 metrics: ', mae_unsupervised_beta10, rmse_unsupervised_beta10, ssim_unsupervised_beta10)\n",
    "    print('unsupervised beta10 avg10 metrics: ', mae_unsupervised_beta10_avg10, rmse_unsupervised_beta10_avg10, ssim_unsupervised_beta10_avg10)\n",
    "    print('unsupervised beta10 avg20 metrics: ', mae_unsupervised_beta10_avg20, rmse_unsupervised_beta10_avg20, ssim_unsupervised_beta10_avg20)\n",
    "    print('unsupervised beta20 metrics: ', mae_unsupervised_beta20, rmse_unsupervised_beta20, ssim_unsupervised_beta20)\n",
    "\n",
    "    ##### TASK2b: calculate lpips in brain\n",
    "    lpips_motion = ff.compute_lpips_3d(condition_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "    lpips_n2n = ff.compute_lpips_3d(noise2noise_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "    lpips_supervised = ff.compute_lpips_3d(supervised_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "\n",
    "    lpips_unsupervised_beta0 = ff.compute_lpips_3d(unsupervised_beta0_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "    lpips_unsupervised_beta0_avg10 = ff.compute_lpips_3d(unsupervised_beta0_avg10_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "    lpips_unsupervised_beta0_avg20 = ff.compute_lpips_3d(unsupervised_beta0_avg20_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "    lpips_unsupervised_beta10 = ff.compute_lpips_3d(unsupervised_beta10_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "    lpips_unsupervised_beta10_avg10 = ff.compute_lpips_3d(unsupervised_beta10_avg10_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "    lpips_unsupervised_beta10_avg20 = ff.compute_lpips_3d(unsupervised_beta10_avg20_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "    lpips_unsupervised_beta20 = ff.compute_lpips_3d(unsupervised_beta20_img_brain, gt_img_brain, max_val = 100, min_val = 0, mask = mask_eroded)\n",
    "\n",
    "    print('lpips motion:', lpips_motion)\n",
    "    print('lpips n2n:', lpips_n2n)\n",
    "    print('lpips supervised:', lpips_supervised)\n",
    "    print('lpips unsupervised beta0:', lpips_unsupervised_beta0)\n",
    "    print('lpips unsupervised beta0 avg10:', lpips_unsupervised_beta0_avg10)\n",
    "    print('lpips unsupervised beta0 avg20:', lpips_unsupervised_beta0_avg20)\n",
    "    print('lpips unsupervised beta10:', lpips_unsupervised_beta10)\n",
    "    print('lpips unsupervised beta10 avg10:', lpips_unsupervised_beta10_avg10)\n",
    "    print('lpips unsupervised beta10 avg20:', lpips_unsupervised_beta10_avg20)\n",
    "    print('lpips unsupervised beta20:', lpips_unsupervised_beta20)\n",
    "\n",
    "    results.append([patient_id, patient_subid, random_n, \n",
    "    mae_motion, rmse_motion, ssim_motion, lpips_motion,\n",
    "    mae_n2n, rmse_n2n, ssim_n2n, lpips_n2n,\n",
    "    mae_supervised, rmse_supervised, ssim_supervised, lpips_supervised,\n",
    "    mae_unsupervised_beta0, rmse_unsupervised_beta0, ssim_unsupervised_beta0, lpips_unsupervised_beta0,\n",
    "    mae_unsupervised_beta0_avg10, rmse_unsupervised_beta0_avg10, ssim_unsupervised_beta0_avg10, lpips_unsupervised_beta0_avg10,\n",
    "    mae_unsupervised_beta0_avg20, rmse_unsupervised_beta0_avg20, ssim_unsupervised_beta0_avg20, lpips_unsupervised_beta0_avg20,\n",
    "    mae_unsupervised_beta10, rmse_unsupervised_beta10, ssim_unsupervised_beta10, lpips_unsupervised_beta10,\n",
    "    mae_unsupervised_beta10_avg10, rmse_unsupervised_beta10_avg10, ssim_unsupervised_beta10_avg10, lpips_unsupervised_beta10_avg10,\n",
    "    mae_unsupervised_beta10_avg20, rmse_unsupervised_beta10_avg20, ssim_unsupervised_beta10_avg20, lpips_unsupervised_beta10_avg20,\n",
    "    mae_unsupervised_beta20, rmse_unsupervised_beta20, ssim_unsupervised_beta20, lpips_unsupervised_beta20])\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame(results, columns = ['patient_id', 'patient_subid', 'random_n', \n",
    "    'mae_motion', 'rmse_motion', 'ssim_motion', 'lpips_motion',\n",
    "    'mae_n2n', 'rmse_n2n', 'ssim_n2n', 'lpips_n2n',\n",
    "    'mae_supervised', 'rmse_supervised', 'ssim_supervised', 'lpips_supervised',\n",
    "    'mae_unsupervised_beta0', 'rmse_unsupervised_beta0', 'ssim_unsupervised_beta0', 'lpips_unsupervised_beta0',\n",
    "    'mae_unsupervised_beta0_avg10', 'rmse_unsupervised_beta0_avg10', 'ssim_unsupervised_beta0_avg10', 'lpips_unsupervised_beta0_avg10',\n",
    "    'mae_unsupervised_beta0_avg20', 'rmse_unsupervised_beta0_avg20', 'ssim_unsupervised_beta0_avg20', 'lpips_unsupervised_beta0_avg20',\n",
    "    'mae_unsupervised_beta10', 'rmse_unsupervised_beta10', 'ssim_unsupervised_beta10', 'lpips_unsupervised_beta10',\n",
    "    'mae_unsupervised_beta10_avg10', 'rmse_unsupervised_beta10_avg10', 'ssim_unsupervised_beta10_avg10', 'lpips_unsupervised_beta10_avg10',\n",
    "    'mae_unsupervised_beta10_avg20', 'rmse_unsupervised_beta10_avg20', 'ssim_unsupervised_beta10_avg20', 'lpips_unsupervised_beta10_avg20',\n",
    "    'mae_unsupervised_beta20', 'rmse_unsupervised_beta20', 'ssim_unsupervised_beta20', 'lpips_unsupervised_beta20'])\n",
    "    file_name = 'quantitative_results.xlsx' \n",
    "    df.to_excel(os.path.join('/mnt/camca_NAS/denoising/models', file_name), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "output:\n",
    "- ```quantitative_results_multiple_inferences.xlsx``` as a spreadsheet\n",
    "- also plot the relation between metrics and the number of inferences averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00214841 0000455418 0\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/vgg.pth\n",
      "when avg num: 1  mae: 3.76296241308645 rmse: 4.830807743033784 ssim: 0.9168411188387703 lpips: 0.1212454092502594\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/vgg.pth\n",
      "when avg num: 2  mae: 2.963731062472677 rmse: 3.8647080851938633 ssim: 0.9453205044432571 lpips: 0.10638526186347008\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/vgg.pth\n",
      "when avg num: 3  mae: 2.636260222847671 rmse: 3.47389378700258 ssim: 0.9554597482995627 lpips: 0.09817960903048516\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /usr/local/lib/python3.8/dist-packages/lpips/weights/v0.1/vgg.pth\n",
      "when avg num: 4  mae: 2.4565547779952883 rmse: 3.266353850163375 ssim: 0.9604475480147776 lpips: 0.09331772834062577\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Documents/CTDenoising_Diffusion_N2N/step5_quantitative.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505533227d/workspace/Documents/CTDenoising_Diffusion_N2N/step5_quantitative.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m img_brain \u001b[39m=\u001b[39m Data_processing\u001b[39m.\u001b[39mcutoff_intensity(img, cutoff_low\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m100\u001b[39m, cutoff_high\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505533227d/workspace/Documents/CTDenoising_Diffusion_N2N/step5_quantitative.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m mae, _, rmsm, _, ssim,_ \u001b[39m=\u001b[39m ff\u001b[39m.\u001b[39mcompare(img_brain, gt_img_brain, cutoff_low \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, cutoff_high \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505533227d/workspace/Documents/CTDenoising_Diffusion_N2N/step5_quantitative.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m lpips \u001b[39m=\u001b[39m ff\u001b[39m.\u001b[39;49mcompute_lpips_3d(img_brain, gt_img_brain, max_val \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, min_val \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505533227d/workspace/Documents/CTDenoising_Diffusion_N2N/step5_quantitative.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mwhen avg num:\u001b[39m\u001b[39m'\u001b[39m, k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m mae:\u001b[39m\u001b[39m'\u001b[39m, mae, \u001b[39m'\u001b[39m\u001b[39mrmse:\u001b[39m\u001b[39m'\u001b[39m, rmsm, \u001b[39m'\u001b[39m\u001b[39mssim:\u001b[39m\u001b[39m'\u001b[39m, ssim, \u001b[39m'\u001b[39m\u001b[39mlpips:\u001b[39m\u001b[39m'\u001b[39m, lpips)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646f636b65725f6578227d@ssh-remote%2B7b22686f73744e616d65223a227a68656e6e6f6e676368656e5f434344535f47505533227d/workspace/Documents/CTDenoising_Diffusion_N2N/step5_quantitative.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m r \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [mae, rmsm, ssim, lpips]\n",
      "File \u001b[0;32m/workspace/Documents/CTDenoising_Diffusion_N2N/functions_collection/functions.py:248\u001b[0m, in \u001b[0;36mcompute_lpips_3d\u001b[0;34m(prediction, ground_truth, mask, max_val, min_val, net_type)\u001b[0m\n\u001b[1;32m    245\u001b[0m     ground_truth \u001b[39m=\u001b[39m (ground_truth \u001b[39m-\u001b[39m min_val) \u001b[39m/\u001b[39m (max_val \u001b[39m-\u001b[39m min_val) \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    247\u001b[0m \u001b[39m# Initialize LPIPS loss model\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m loss_fn \u001b[39m=\u001b[39m lpips\u001b[39m.\u001b[39;49mLPIPS(net\u001b[39m=\u001b[39;49mnet_type)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    250\u001b[0m lpips_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m    252\u001b[0m \u001b[39m# Loop through each slice along the z-axis\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/lpips/lpips.py:84\u001b[0m, in \u001b[0;36mLPIPS.__init__\u001b[0;34m(self, pretrained, net, version, lpips, spatial, pnet_rand, pnet_tune, use_dropout, model_path, eval_mode, verbose)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchns \u001b[39m=\u001b[39m [\u001b[39m64\u001b[39m,\u001b[39m128\u001b[39m,\u001b[39m256\u001b[39m,\u001b[39m384\u001b[39m,\u001b[39m384\u001b[39m,\u001b[39m512\u001b[39m,\u001b[39m512\u001b[39m]\n\u001b[1;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mL \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchns)\n\u001b[0;32m---> 84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet \u001b[39m=\u001b[39m net_type(pretrained\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet_rand, requires_grad\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet_tune)\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m(lpips):\n\u001b[1;32m     87\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin0 \u001b[39m=\u001b[39m NetLinLayer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchns[\u001b[39m0\u001b[39m], use_dropout\u001b[39m=\u001b[39muse_dropout)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/lpips/pretrained_networks.py:99\u001b[0m, in \u001b[0;36mvgg16.__init__\u001b[0;34m(self, requires_grad, pretrained)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     98\u001b[0m     \u001b[39msuper\u001b[39m(vgg16, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 99\u001b[0m     vgg_pretrained_features \u001b[39m=\u001b[39m tv\u001b[39m.\u001b[39;49mvgg16(pretrained\u001b[39m=\u001b[39;49mpretrained)\u001b[39m.\u001b[39mfeatures\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSequential()\n\u001b[1;32m    101\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSequential()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing \u001b[39m\u001b[39m{\u001b[39;00msequence_to_str(\u001b[39mtuple\u001b[39m(keyword_only_kwargs\u001b[39m.\u001b[39mkeys()),\u001b[39m \u001b[39mseparate_last\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mand \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m as positional \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[39m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m builder(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/vgg.py:433\u001b[0m, in \u001b[0;36mvgg16\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"VGG-16 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m    :members:\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    431\u001b[0m weights \u001b[39m=\u001b[39m VGG16_Weights\u001b[39m.\u001b[39mverify(weights)\n\u001b[0;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m _vgg(\u001b[39m\"\u001b[39;49m\u001b[39mD\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m, weights, progress, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/vgg.py:103\u001b[0m, in \u001b[0;36m_vgg\u001b[0;34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m weights\u001b[39m.\u001b[39mmeta[\u001b[39m\"\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         _ovewrite_named_param(kwargs, \u001b[39m\"\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(weights\u001b[39m.\u001b[39mmeta[\u001b[39m\"\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m--> 103\u001b[0m model \u001b[39m=\u001b[39m VGG(make_layers(cfgs[cfg], batch_norm\u001b[39m=\u001b[39;49mbatch_norm), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(weights\u001b[39m.\u001b[39mget_state_dict(progress\u001b[39m=\u001b[39mprogress, check_hash\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/vgg.py:44\u001b[0m, in \u001b[0;36mVGG.__init__\u001b[0;34m(self, features, num_classes, init_weights, dropout)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m features\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mAdaptiveAvgPool2d((\u001b[39m7\u001b[39m, \u001b[39m7\u001b[39m))\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m---> 44\u001b[0m     nn\u001b[39m.\u001b[39;49mLinear(\u001b[39m512\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m7\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m7\u001b[39;49m, \u001b[39m4096\u001b[39;49m),\n\u001b[1;32m     45\u001b[0m     nn\u001b[39m.\u001b[39mReLU(\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     46\u001b[0m     nn\u001b[39m.\u001b[39mDropout(p\u001b[39m=\u001b[39mdropout),\n\u001b[1;32m     47\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m4096\u001b[39m, \u001b[39m4096\u001b[39m),\n\u001b[1;32m     48\u001b[0m     nn\u001b[39m.\u001b[39mReLU(\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     49\u001b[0m     nn\u001b[39m.\u001b[39mDropout(p\u001b[39m=\u001b[39mdropout),\n\u001b[1;32m     50\u001b[0m     nn\u001b[39m.\u001b[39mLinear(\u001b[39m4096\u001b[39m, num_classes),\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m init_weights:\n\u001b[1;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodules():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:104\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 104\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:110\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/init.py:460\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    458\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound, generator\u001b[39m=\u001b[39;49mgenerator)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_inference = [] \n",
    "for i in range(0,n.shape[0]):\n",
    "    patient_id = patient_id_list[n[i]]\n",
    "    patient_subid = patient_subid_list[n[i]]\n",
    "    random_n = random_num_list[n[i]]\n",
    "    print(patient_id, patient_subid, random_n)\n",
    "    r = [patient_id, patient_subid, random_n]\n",
    "\n",
    "    gt_file = os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta0/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61_1/gt_img.nii.gz')\n",
    "    gt_img = nb.load(gt_file).get_fdata()\n",
    "    gt_img_brain = Data_processing.cutoff_intensity(gt_img, cutoff_low=-100, cutoff_high=100)\n",
    "\n",
    "    files = ff.sort_timeframe(ff.find_all_target_files(['pred_img_scans*'], os.path.join('/mnt/camca_NAS/denoising/models/unsupervised_gaussian_current_beta0/pred_images', patient_id, patient_subid,'random_'+str(random_n), 'epoch61avg')),2,'s','.')\n",
    "    \n",
    "    for k in range(0,len(files)):\n",
    "        img = nb.load(files[k]).get_fdata() \n",
    "        img_brain = Data_processing.cutoff_intensity(img, cutoff_low=-100, cutoff_high=100)\n",
    "        mae, _, rmsm, _, ssim,_ = ff.compare(img_brain, gt_img_brain, cutoff_low = 0, cutoff_high = 100)\n",
    "        lpips = ff.compute_lpips_3d(img_brain, gt_img_brain, max_val = 100, min_val = 0)\n",
    "\n",
    "        avg_times = ff.find_timeframe(files[k], 2, 's', '.')\n",
    "        print('when avg num:', avg_times, ' mae:', mae, 'rmse:', rmsm, 'ssim:', ssim, 'lpips:', lpips)\n",
    "        r += [mae, rmsm, ssim, lpips]\n",
    "\n",
    "    results_inference.append(r)\n",
    "\n",
    "    columns = ['patient_id', 'patient_subid', 'random_n']\n",
    "    for k in range(0,len(files)):\n",
    "        avg_times = ff.find_timeframe(files[k], 2, 's', '.')\n",
    "        columns += ['mae_inference'+str(avg_times), 'rmse_inference'+str(avg_times), 'ssim_inference'+str(avg_times), 'lpips_inference'+str(avg_times)]\n",
    "    df = pd.DataFrame(results_inference, columns = columns)\n",
    "    file_name = 'quantitative_results_multiple_inferences1.xlsx' \n",
    "    df.to_excel(os.path.join('/mnt/camca_NAS/denoising/models', file_name), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
