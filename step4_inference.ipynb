{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "You should prepare the following things before running this step. \n",
    "\n",
    "1. **trained model** from ```step3.ipynb``` \n",
    "\n",
    "2. **A patient list** that emunarates the dataset \n",
    "   - check ```step2.ipynb```\n",
    "   - for example data: as we said in previous steps, we are going to validate our models on type 2 noise. so please use  ```example_data/Patient_lists/patient_list_unsupervised_gaussian.xlsx``` for unsupervised learning.\n",
    "\n",
    "3. bins for **histogram equalization**\n",
    "    - provided in ```/help_data```\n",
    "\n",
    "---\n",
    "\n",
    "## Task: Inference\n",
    "\n",
    "- first: we do inference for multiple times (section \"step 5.prediction\" in the script) \n",
    "- second: we take the average (section \"step6.average\" in the sript) to be the final product.\n",
    "\n",
    "---\n",
    "\n",
    "### Docker environment\n",
    "Please use `docker/docker_pytorch`, it will build a pytorch docker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/Documents/CTDenoising_Diffusion_N2N/denoising_diffusion_pytorch/denoising_diffusion_pytorch/conditional_diffusion.py:1000: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/Documents')\n",
    "import os\n",
    "import torch\n",
    "import numpy as np \n",
    "import nibabel as nb\n",
    "import CTDenoising_Diffusion_N2N.denoising_diffusion_pytorch.denoising_diffusion_pytorch.conditional_diffusion as ddpm\n",
    "import CTDenoising_Diffusion_N2N.functions_collection as ff\n",
    "import CTDenoising_Diffusion_N2N.Build_lists.Build_list as Build_list\n",
    "import CTDenoising_Diffusion_N2N.Generator as Generator\n",
    "\n",
    "main_path = '/mnt/camca_NAS/denoising/' # replace with your own path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: define trial name and trained model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_unsupervised_gaussian_beta0\n"
     ]
    }
   ],
   "source": [
    "supervision = 'unsupervised' # 'unsupervised' or 'supervised'\n",
    "noise_type = 'possion' if supervision == 'supervised' else 'gaussian'\n",
    "beta = 0 # by default\n",
    "\n",
    "trial_name = 'model_'+supervision + '_' + noise_type + '_beta' + str(beta)\n",
    "print(trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 61\n",
    "trained_model_filename = os.path.join(main_path,'models', trial_name, 'models/model-' + str(epoch) + '.pt')\n",
    "save_folder = os.path.join(main_path,'models', trial_name,'pred_images')\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: set default parameters\n",
    "usually you don't need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dimension = '2D'\n",
    "condition_channel = 1 if (supervision == 'supervised') or ('mean' in trial_name) else 2\n",
    "image_size = [512,512]\n",
    "num_patches_per_slice = 2\n",
    "patch_size = [128,128]\n",
    "\n",
    "objective = 'pred_x0'\n",
    "\n",
    "sampling_timesteps = 100\n",
    "\n",
    "histogram_equalization = True\n",
    "background_cutoff = -1000\n",
    "maximum_cutoff = 2000\n",
    "normalize_factor = 'equation'\n",
    "clip_range = [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: define patient list\n",
    "test on type 2 noise  (Gaussian noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number: 1\n"
     ]
    }
   ],
   "source": [
    "build_sheet =  Build_list.Build(os.path.join(main_path, 'example_data/Patient_lists','patient_list_unsupervised_gaussian.xlsx'))\n",
    "_,patient_id_list,patient_subid_list,random_num_list, condition_list, x0_list = build_sheet.__build__(batch_list = [0])  # for the purpose of example, we test on the same training case\n",
    "# n = ff.get_X_numbers_in_interval(total_number = patient_id_list.shape[0],start_number = 0,end_number = 1, interval = 2) # each case has two simulations, we do on the first one as example\n",
    "print('total number:', patient_id_list.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4: define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is ddim sampling True\n"
     ]
    }
   ],
   "source": [
    "model = ddpm.Unet(\n",
    "    problem_dimension = problem_dimension,\n",
    "    init_dim = 64,\n",
    "    out_dim = 1,\n",
    "    channels = 1, \n",
    "    conditional_diffusion = True,\n",
    "    condition_channels = condition_channel,\n",
    "\n",
    "    downsample_list = (True, True, True, False),\n",
    "    upsample_list = (True, True, True, False),\n",
    "    full_attn = (None, None, False, True),) # make sure these settings are the same as step 3 training\n",
    "\n",
    "\n",
    "diffusion_model = ddpm.GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = image_size,\n",
    "    timesteps = 1000,           # number of steps\n",
    "    sampling_timesteps = sampling_timesteps,    \n",
    "    ddim_sampling_eta = 1., # don't change!!\n",
    "    force_ddim = False,\n",
    "    auto_normalize=False,\n",
    "    objective = objective,\n",
    "    clip_or_not = True, \n",
    "    clip_range = clip_range, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 5: Prediction (doing inference for multiple times)\n",
    "to minimize data storage, we only evaluated on the middle 50 slices (slice 30 - 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 00004038 0000455420 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition_file: /mnt/camca_NAS/denoising/example_data/simulation/00004038/0000455420/gaussian_random_0/recon.nii.gz shape:  (512, 512, 100)\n",
      "x0_file: /mnt/camca_NAS/denoising/example_data/fixedCT/00004038/0000455420/img_thinslice_partial.nii.gz shape: (512, 512, 100)\n",
      "iteration: 1\n",
      "histogram equalization:  True\n",
      "model device:  cuda:0\n",
      "using DDIM, eta:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 100/100 [00:06<00:00, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_img_slice shape:  (512, 512)\n",
      "using DDIM, eta:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 100/100 [00:06<00:00, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_img_slice shape:  (512, 512)\n",
      "final image shape:  (512, 512, 2)\n",
      "(512, 512, 2)\n",
      "iteration: 2\n",
      "histogram equalization:  True\n",
      "model device:  cuda:0\n",
      "using DDIM, eta:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 100/100 [00:06<00:00, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_img_slice shape:  (512, 512)\n",
      "using DDIM, eta:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 100/100 [00:06<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_img_slice shape:  (512, 512)\n",
      "final image shape:  (512, 512, 2)\n",
      "(512, 512, 2)\n"
     ]
    }
   ],
   "source": [
    "slice_range = [30,80] # the range of slices to be used\n",
    "inference_times = 20\n",
    "\n",
    "for i in range(0,patient_id_list.shape[0]):\n",
    "    patient_id = patient_id_list[i]\n",
    "    patient_subid = patient_subid_list[i]\n",
    "    random_num = random_num_list[i]\n",
    "    x0_file = x0_list[i]\n",
    "    condition_file = condition_list[i]\n",
    "\n",
    "    print(i,patient_id, patient_subid, random_num)\n",
    "\n",
    "    # get the condition image (original noisy image)\n",
    "    print('condition_file:', condition_file, 'shape: ', nb.load(condition_file).get_fdata().shape)\n",
    "    condition_img = nb.load(condition_file).get_fdata()[:,:,slice_range[0]:slice_range[1]]\n",
    "    affine = nb.load(condition_file).affine\n",
    "    shape = condition_img.shape\n",
    "\n",
    "    # get the ground truth image (if no ground truth image (in the real-world cases), just comment out the following lines)\n",
    "    gt_img = nb.load(x0_file)\n",
    "    print('x0_file:', x0_file, 'shape:', gt_img.get_fdata().shape)\n",
    "    gt_img = gt_img.get_fdata()[:,:,slice_range[0]:slice_range[1]]\n",
    "\n",
    "    for iteration in range(1,1+inference_times):\n",
    "        print('iteration:', iteration)\n",
    "\n",
    "        # make folders\n",
    "        ff.make_folder([os.path.join(save_folder, patient_id), os.path.join(save_folder, patient_id, patient_subid), os.path.join(save_folder, patient_id, patient_subid, 'random_' + str(random_num))])\n",
    "        save_folder_case = os.path.join(save_folder, patient_id, patient_subid, 'random_' + str(random_num), 'epoch' + str(epoch)+'_'+str(iteration))\n",
    "        os.makedirs(save_folder_case, exist_ok=True)\n",
    "\n",
    "        if os.path.isfile(os.path.join(save_folder_case, 'pred_img.nii.gz')):\n",
    "            print('already done')\n",
    "            continue\n",
    "\n",
    "        # generator\n",
    "        generator = Generator.Dataset_2D(\n",
    "            supervision = supervision,\n",
    "\n",
    "            img_list = np.array([x0_file]),\n",
    "            condition_list = np.array([condition_file]),\n",
    "            image_size = image_size,\n",
    "\n",
    "            num_slices_per_image = slice_range[1] - slice_range[0],\n",
    "            random_pick_slice = False,\n",
    "            slice_range = [slice_range[0], slice_range[1]],\n",
    "\n",
    "            histogram_equalization = histogram_equalization,\n",
    "            bins = np.load('./help_data/histogram_equalization/bins.npy'),\n",
    "            bins_mapped = np.load('./help_data/histogram_equalization/bins_mapped.npy'),\n",
    "                \n",
    "            background_cutoff = background_cutoff,\n",
    "            maximum_cutoff = maximum_cutoff,\n",
    "            normalize_factor = normalize_factor,)\n",
    "\n",
    "        # sample:\n",
    "        sampler = ddpm.Sampler(diffusion_model,generator,batch_size = 1)\n",
    "\n",
    "        pred_img = sampler.sample_2D(trained_model_filename, condition_img)\n",
    "        print(pred_img.shape)\n",
    "    \n",
    "        # save\n",
    "        nb.save(nb.Nifti1Image(pred_img, affine), os.path.join(save_folder_case, 'pred_img.nii.gz'))\n",
    "\n",
    "        if iteration == 1:\n",
    "            nb.save(nb.Nifti1Image(condition_img, affine), os.path.join(save_folder_case, 'condition_img.nii.gz'))\n",
    "            nb.save(nb.Nifti1Image(gt_img, affine), os.path.join(save_folder_case, 'gt_img.nii.gz'))\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 6: average the results of multiple inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 00004038 0000455420 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition_file: /mnt/camca_NAS/denoising/example_data/simulation/00004038/0000455420/gaussian_random_0/recon.nii.gz shape:  (512, 512, 100)\n",
      "['/mnt/camca_NAS/denoising/models/model_unsupervised_gaussian_beta0/pred_images/00004038/0000455420/random_0/epoch61_1'\n",
      " '/mnt/camca_NAS/denoising/models/model_unsupervised_gaussian_beta0/pred_images/00004038/0000455420/random_0/epoch61_2']\n",
      "avg_num: 2\n",
      "predict_num: 2\n",
      "file: /mnt/camca_NAS/denoising/models/model_unsupervised_gaussian_beta0/pred_images/00004038/0000455420/random_0/epoch61_1\n",
      "file: /mnt/camca_NAS/denoising/models/model_unsupervised_gaussian_beta0/pred_images/00004038/0000455420/random_0/epoch61_2\n"
     ]
    }
   ],
   "source": [
    "slice_range = [30,80] # the range of slices to be used\n",
    "inference_avg_scans = [10,20] # avg 10 or 20 inference results\n",
    "\n",
    "for i in range(0,patient_id_list.shape[0]):\n",
    "    patient_id = patient_id_list[i]\n",
    "    patient_subid = patient_subid_list[i]\n",
    "    random_num = random_num_list[i]\n",
    "    x0_file = x0_list[i]\n",
    "    condition_file = condition_list[i]\n",
    "\n",
    "    print(i,patient_id, patient_subid, random_num)\n",
    "\n",
    "    save_folder_avg = os.path.join(save_folder, patient_id, patient_subid, 'random_' + str(random_num), 'epoch' + str(epoch)+'avg'); os.makedirs(save_folder_avg, exist_ok=True)\n",
    "\n",
    "    # get the condition image (original noisy image)\n",
    "    print('condition_file:', condition_file, 'shape: ', nb.load(condition_file).get_fdata().shape)\n",
    "    condition_img = nb.load(condition_file).get_fdata()[:,:,slice_range[0]:slice_range[1]]\n",
    "    affine = nb.load(condition_file).affine\n",
    "    shape = condition_img.shape\n",
    "        \n",
    "    made_predicts = ff.sort_timeframe(ff.find_all_target_files(['epoch' + str(epoch)+'_*'], os.path.join(save_folder, patient_id, patient_subid, 'random_' + str(random_num))),0,'_','/')\n",
    "    print(made_predicts)\n",
    "    total_predicts = len(made_predicts)\n",
    "\n",
    "    loaded_data = np.zeros((shape[0], shape[1], shape[2], total_predicts))\n",
    "    for j in range(total_predicts):\n",
    "        loaded_data[:,:,:,j] = nb.load(os.path.join(made_predicts[j],'pred_img.nii.gz')).get_fdata()\n",
    "\n",
    "    for avg_num in inference_avg_scans:\n",
    "        print('avg_num:', avg_num)\n",
    "        predicts_avg = np.zeros((shape[0], shape[1], shape[2], avg_num))\n",
    "        print('predict_num:', avg_num)\n",
    "        for j in range(avg_num):\n",
    "            print('file:', made_predicts[j])\n",
    "            predicts_avg[:,:,:,j] = loaded_data[:,:,:,j]\n",
    "        # average across last axis\n",
    "        predicts_avg = np.mean(predicts_avg, axis = -1)\n",
    "        nb.save(nb.Nifti1Image(predicts_avg, affine), os.path.join(save_folder_avg, 'pred_img_scans' + str(avg_num) + '.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
